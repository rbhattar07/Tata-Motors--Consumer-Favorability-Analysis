{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1619,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\14708\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from spacy) (1.24.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\14708\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\14708\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\14708\\\\anaconda3\\\\python.exe'"
      ]
     },
     "execution_count": 1621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1680,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\14708\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\14708\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\14708\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\14708\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import plotly.express as px \n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'\n",
    "\n",
    "pd.set_option('display.max_columns', 25)\n",
    "pd.set_option('display.max_rows', 1520)\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1623,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df=pd.read_csv(r\"C:\\Users\\14708\\Downloads\\Tata and Hyundai cars - data.xlsx - Data.csv\")\n",
    "df = main_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Headline</th>\n",
       "      <th>URL</th>\n",
       "      <th>Opening Text</th>\n",
       "      <th>Hit Sentence</th>\n",
       "      <th>Source</th>\n",
       "      <th>Influencer</th>\n",
       "      <th>Country</th>\n",
       "      <th>Subregion</th>\n",
       "      <th>Language</th>\n",
       "      <th>Reach</th>\n",
       "      <th>Desktop Reach</th>\n",
       "      <th>Mobile Reach</th>\n",
       "      <th>Twitter Social Echo</th>\n",
       "      <th>Facebook Social Echo</th>\n",
       "      <th>Reddit Social Echo</th>\n",
       "      <th>National Viewership</th>\n",
       "      <th>Engagement</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28-Sep-2023 02:08PM</td>\n",
       "      <td>Kia और Hyundai ने वापस बुलाईं अपनी 35 लाख कारे...</td>\n",
       "      <td>https://hindi.pardaphash.com/kia-and-hyundai-r...</td>\n",
       "      <td>Kia and Hyundai Recalled Cars: वाहन निर्माता क...</td>\n",
       "      <td>Kia and Hyundai Recalled Cars: वाहन निर्माता क...</td>\n",
       "      <td>Pardaphash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Hindi</td>\n",
       "      <td>31307.0</td>\n",
       "      <td>1279.0</td>\n",
       "      <td>30028.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Hyundai,Cars</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28-Sep-2023 01:46PM</td>\n",
       "      <td>Tata Punch EV spied, may compete with MG Comen...</td>\n",
       "      <td>https://economictimes.indiatimes.com/industry/...</td>\n",
       "      <td>Tata Motors is reportedly developing an electr...</td>\n",
       "      <td>... Punch EV is expected to compete with the M...</td>\n",
       "      <td>The Economic Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>National</td>\n",
       "      <td>English</td>\n",
       "      <td>15403059.0</td>\n",
       "      <td>6171732.0</td>\n",
       "      <td>9231327.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Tata,cars</td>\n",
       "      <td>Maharashtra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28-Sep-2023 01:30PM</td>\n",
       "      <td>Tata Avinya Reviews On Road Price 2023 Mileage...</td>\n",
       "      <td>https://badisoch.in/automobile-news/tata-aviny...</td>\n",
       "      <td>... (adsbygoogle = window.adsbygoogle || []).p...</td>\n",
       "      <td>... , despite most carmakers focusing on large...</td>\n",
       "      <td>Badi Soch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>1128280.0</td>\n",
       "      <td>171549.0</td>\n",
       "      <td>956731.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>cars,Tata</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28-Sep-2023 01:24PM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://twitter.com/automobilindia8/statuses/17...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Engine Fire-Related Risks Force Kia &amp; Hyundai ...</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>@automobilindia8</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Hyundai,Cars,hyundai</td>\n",
       "      <td>Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28-Sep-2023 01:22PM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://twitter.com/MalakpetD/statuses/17073021...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@srinualavilli @BMTC_BENGALURU Mahindra, Tata,...</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>@MalakpetD</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>506.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Tata,cars</td>\n",
       "      <td>Telangana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date                                           Headline  \\\n",
       "0  28-Sep-2023 02:08PM  Kia और Hyundai ने वापस बुलाईं अपनी 35 लाख कारे...   \n",
       "1  28-Sep-2023 01:46PM  Tata Punch EV spied, may compete with MG Comen...   \n",
       "2  28-Sep-2023 01:30PM  Tata Avinya Reviews On Road Price 2023 Mileage...   \n",
       "3  28-Sep-2023 01:24PM                                                NaN   \n",
       "4  28-Sep-2023 01:22PM                                                NaN   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://hindi.pardaphash.com/kia-and-hyundai-r...   \n",
       "1  https://economictimes.indiatimes.com/industry/...   \n",
       "2  https://badisoch.in/automobile-news/tata-aviny...   \n",
       "3  http://twitter.com/automobilindia8/statuses/17...   \n",
       "4  http://twitter.com/MalakpetD/statuses/17073021...   \n",
       "\n",
       "                                        Opening Text  \\\n",
       "0  Kia and Hyundai Recalled Cars: वाहन निर्माता क...   \n",
       "1  Tata Motors is reportedly developing an electr...   \n",
       "2  ... (adsbygoogle = window.adsbygoogle || []).p...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                        Hit Sentence              Source  \\\n",
       "0  Kia and Hyundai Recalled Cars: वाहन निर्माता क...          Pardaphash   \n",
       "1  ... Punch EV is expected to compete with the M...  The Economic Times   \n",
       "2  ... , despite most carmakers focusing on large...           Badi Soch   \n",
       "3  Engine Fire-Related Risks Force Kia & Hyundai ...             Twitter   \n",
       "4  @srinualavilli @BMTC_BENGALURU Mahindra, Tata,...             Twitter   \n",
       "\n",
       "         Influencer Country Subregion Language       Reach  Desktop Reach  \\\n",
       "0               NaN   India     Delhi    Hindi     31307.0         1279.0   \n",
       "1               NaN   India  National  English  15403059.0      6171732.0   \n",
       "2               NaN   India       NaN  English   1128280.0       171549.0   \n",
       "3  @automobilindia8   India       NaN  English         2.0            NaN   \n",
       "4        @MalakpetD   India       NaN  English       506.0            NaN   \n",
       "\n",
       "   Mobile Reach  Twitter Social Echo  Facebook Social Echo  \\\n",
       "0       30028.0                  0.0                   0.0   \n",
       "1     9231327.0                  0.0                   0.0   \n",
       "2      956731.0                  0.0                   0.0   \n",
       "3           NaN                  NaN                   NaN   \n",
       "4           NaN                  NaN                   NaN   \n",
       "\n",
       "   Reddit Social Echo  National Viewership  Engagement Sentiment  \\\n",
       "0                 0.0                    0         NaN   Neutral   \n",
       "1                 0.0                    0         NaN   Neutral   \n",
       "2                 0.0                    0         NaN   Neutral   \n",
       "3                 NaN                    0         0.0   Neutral   \n",
       "4                 NaN                    0         0.0   Neutral   \n",
       "\n",
       "               Keywords        State  \n",
       "0          Hyundai,Cars        Delhi  \n",
       "1             Tata,cars  Maharashtra  \n",
       "2             cars,Tata          NaN  \n",
       "3  Hyundai,Cars,hyundai    Karnataka  \n",
       "4             Tata,cars    Telangana  "
      ]
     },
     "execution_count": 1624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                                                  28-Sep-2023 02:08PM\n",
       "Headline                Kia और Hyundai ने वापस बुलाईं अपनी 35 लाख कारे...\n",
       "URL                     https://hindi.pardaphash.com/kia-and-hyundai-r...\n",
       "Opening Text            Kia and Hyundai Recalled Cars: वाहन निर्माता क...\n",
       "Hit Sentence            Kia and Hyundai Recalled Cars: वाहन निर्माता क...\n",
       "Source                                                         Pardaphash\n",
       "Influencer                                                            NaN\n",
       "Country                                                             India\n",
       "Subregion                                                           Delhi\n",
       "Language                                                            Hindi\n",
       "Reach                                                             31307.0\n",
       "Desktop Reach                                                      1279.0\n",
       "Mobile Reach                                                      30028.0\n",
       "Twitter Social Echo                                                   0.0\n",
       "Facebook Social Echo                                                  0.0\n",
       "Reddit Social Echo                                                    0.0\n",
       "National Viewership                                                     0\n",
       "Engagement                                                            NaN\n",
       "Sentiment                                                         Neutral\n",
       "Keywords                                                     Hyundai,Cars\n",
       "State                                                               Delhi\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 1625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1528 entries, 0 to 1527\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Date                  1528 non-null   object \n",
      " 1   Headline              710 non-null    object \n",
      " 2   URL                   1528 non-null   object \n",
      " 3   Opening Text          710 non-null    object \n",
      " 4   Hit Sentence          1528 non-null   object \n",
      " 5   Source                1528 non-null   object \n",
      " 6   Influencer            1065 non-null   object \n",
      " 7   Country               1528 non-null   object \n",
      " 8   Subregion             321 non-null    object \n",
      " 9   Language              1528 non-null   object \n",
      " 10  Reach                 1519 non-null   float64\n",
      " 11  Desktop Reach         710 non-null    float64\n",
      " 12  Mobile Reach          710 non-null    float64\n",
      " 13  Twitter Social Echo   710 non-null    float64\n",
      " 14  Facebook Social Echo  710 non-null    float64\n",
      " 15  Reddit Social Echo    710 non-null    float64\n",
      " 16  National Viewership   1528 non-null   int64  \n",
      " 17  Engagement            352 non-null    float64\n",
      " 18  Sentiment             1528 non-null   object \n",
      " 19  Keywords              1528 non-null   object \n",
      " 20  State                 1028 non-null   object \n",
      "dtypes: float64(7), int64(1), object(13)\n",
      "memory usage: 250.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Headline', 'URL', 'Opening Text', 'Hit Sentence', 'Source',\n",
       "       'Influencer', 'Country', 'Subregion', 'Language', 'Reach',\n",
       "       'Desktop Reach', 'Mobile Reach', 'Twitter Social Echo',\n",
       "       'Facebook Social Echo', 'Reddit Social Echo', 'National Viewership',\n",
       "       'Engagement', 'Sentiment', 'Keywords', 'State'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_cols: Index(['Date', 'Headline', 'URL', 'Opening Text', 'Hit Sentence', 'Source',\n",
      "       'Influencer', 'Country', 'Subregion', 'Language', 'Sentiment',\n",
      "       'Keywords', 'State'],\n",
      "      dtype='object')\n",
      "numeric_cols: Index(['Reach', 'Desktop Reach', 'Mobile Reach', 'Twitter Social Echo',\n",
      "       'Facebook Social Echo', 'Reddit Social Echo', 'National Viewership',\n",
      "       'Engagement'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = df.select_dtypes('object').columns\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print('categorical_cols:', categorical_cols)\n",
    "print('numeric_cols:', numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for categorical columns\n",
    "def val_count(dataframe, column):\n",
    "    return dataframe[column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twitter                818\n",
       "India Posts English     89\n",
       "The Times of India      32\n",
       "Cartoq                  21\n",
       "Todays Chronic          14\n",
       "                      ... \n",
       "TechCodex                1\n",
       "Prameyanews              1\n",
       "Orissa Diary             1\n",
       "ODISHABARTA.COM          1\n",
       "NFAPost                  1\n",
       "Name: Source, Length: 230, dtype: int64"
      ]
     },
     "execution_count": 1630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_count(df,'Source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India    1528\n",
      "Name: Country, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(val_count(df,'Country'))\n",
    "\n",
    "# Will delete this column as the only country in the dataset is India\n",
    "df.drop(columns='Country', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mumbai            87\n",
       "Uttar Pradesh     75\n",
       "Delhi             52\n",
       "Madhya Pradesh    21\n",
       "National          18\n",
       "Maharashtra       14\n",
       "West Bengal       10\n",
       "Haryana            8\n",
       "Kerala             7\n",
       "Karnataka          5\n",
       "Rajasthan          4\n",
       "Orissa             4\n",
       "Chennai            4\n",
       "Gujarat            3\n",
       "J&K                2\n",
       "Punjab             2\n",
       "Jharkhand          1\n",
       "Chattisgarh        1\n",
       "Kolkata            1\n",
       "Uttrakhand         1\n",
       "Bangalore          1\n",
       "Name: Subregion, dtype: int64"
      ]
     },
     "execution_count": 1632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_count(df,'Subregion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tamil Nadu           264\n",
       "Maharashtra          218\n",
       "Delhi                 96\n",
       "Uttar Pradesh         95\n",
       "Karnataka             71\n",
       "Kerala                43\n",
       "Gujarat               35\n",
       "Telangana             33\n",
       "Madhya Pradesh        28\n",
       "Rajasthan             25\n",
       "West Bengal           21\n",
       "Haryana               21\n",
       "Puducherry            13\n",
       "Odisha                11\n",
       "Andhra Pradesh        10\n",
       "Chandigarh             9\n",
       "Jharkhand              8\n",
       "Punjab                 5\n",
       "Bihar                  5\n",
       "Jammu and Kashmir      4\n",
       "Chhattisgarh           4\n",
       "Assam                  2\n",
       "Uttarakhand            2\n",
       "Arunachal Pradesh      1\n",
       "Tripura                1\n",
       "Goa                    1\n",
       "Mizoram                1\n",
       "Himachal Pradesh       1\n",
       "Name: State, dtype: int64"
      ]
     },
     "execution_count": 1633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_count(df,'State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Column null values (count): 500\n",
      "Subregion Column null values (count): 1207\n",
      "New Count State: 497\n",
      "New Count Subregion: 497\n"
     ]
    }
   ],
   "source": [
    "''' The variables state and subregion are supposed to contain specific & seperate type of data but if we look closely then most of these values in both of\n",
    "these columns contain information related to the state from where the source is from, so lets fill all the null values in these two columns by \n",
    "refering them to each other'''\n",
    "\n",
    "print('State Column null values (count):',df['State'].isna().sum())\n",
    "print('Subregion Column null values (count):',df['Subregion'].isna().sum())\n",
    "\n",
    "df['Subregion']=df['Subregion'].fillna(df['State'])\n",
    "df['State']=df['State'].fillna(df['Subregion'])\n",
    "\n",
    "print('New Count State:', df['State'].isna().sum())\n",
    "print('New Count Subregion:', df['Subregion'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tamil Nadu           248\n",
       "Maharashtra          125\n",
       "Delhi                102\n",
       "Uttar Pradesh         95\n",
       "Mumbai                87\n",
       "Karnataka             69\n",
       "Kerala                43\n",
       "Gujarat               34\n",
       "Telangana             33\n",
       "Madhya Pradesh        28\n",
       "Rajasthan             24\n",
       "Haryana               21\n",
       "West Bengal           20\n",
       "National              18\n",
       "Puducherry            13\n",
       "Andhra Pradesh        10\n",
       "Chandigarh             9\n",
       "Jharkhand              8\n",
       "Odisha                 7\n",
       "Punjab                 5\n",
       "Bihar                  5\n",
       "Orissa                 4\n",
       "Chennai                4\n",
       "Chhattisgarh           3\n",
       "Assam                  2\n",
       "Jammu and Kashmir      2\n",
       "J&K                    2\n",
       "Arunachal Pradesh      1\n",
       "Uttrakhand             1\n",
       "Uttarakhand            1\n",
       "Tripura                1\n",
       "Goa                    1\n",
       "Chattisgarh            1\n",
       "Mizoram                1\n",
       "Himachal Pradesh       1\n",
       "Kolkata                1\n",
       "Bangalore              1\n",
       "Name: Subregion, dtype: int64"
      ]
     },
     "execution_count": 1635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_count(df,'Subregion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tamil Nadu           264\n",
       "Maharashtra          218\n",
       "Delhi                 96\n",
       "Uttar Pradesh         95\n",
       "Karnataka             71\n",
       "Kerala                43\n",
       "Gujarat               35\n",
       "Telangana             33\n",
       "Madhya Pradesh        28\n",
       "Rajasthan             25\n",
       "West Bengal           21\n",
       "Haryana               21\n",
       "Puducherry            13\n",
       "Odisha                11\n",
       "Andhra Pradesh        10\n",
       "Chandigarh             9\n",
       "Jharkhand              8\n",
       "Punjab                 5\n",
       "Bihar                  5\n",
       "Jammu and Kashmir      4\n",
       "Chhattisgarh           4\n",
       "National               3\n",
       "Assam                  2\n",
       "Uttarakhand            2\n",
       "Arunachal Pradesh      1\n",
       "Tripura                1\n",
       "Goa                    1\n",
       "Mizoram                1\n",
       "Himachal Pradesh       1\n",
       "Name: State, dtype: int64"
      ]
     },
     "execution_count": 1636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_count(df,'State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1637,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most ideal thing to do will be to remove the sub region column\n",
    "df.drop(columns='Subregion', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English    1507\n",
       "Hindi        14\n",
       "Bengali       6\n",
       "Marathi       1\n",
       "Name: Language, dtype: int64"
      ]
     },
     "execution_count": 1638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_count(df, 'Language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     850\n",
       "Positive    420\n",
       "Negative    258\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 1639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_count(df,'Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cars,Tata                 270\n",
       "Cars,Tata                 267\n",
       "Hyundai,cars              255\n",
       "Tata,cars                 242\n",
       "cars,Hyundai              152\n",
       "                         ... \n",
       "Hyundai,cars,Tata,Cars      1\n",
       "cars,Tata,TATA,Hyundai      1\n",
       "cars,Hyundai,Cars,Tata      1\n",
       "Hyundai,Cars,hyundai        1\n",
       "Cars,Tata,cars,Hyundai      1\n",
       "Name: Keywords, Length: 65, dtype: int64"
      ]
     },
     "execution_count": 1640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_count(df,'Keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hyundai,Cars', 'Tata,cars', 'cars,Tata', 'Hyundai,Cars,hyundai',\n",
       "       'Hyundai,cars', 'cars,Hyundai', 'Cars,Tata', 'CARS,Hyundai',\n",
       "       'Hyundai,cars,Cars', 'cars,Tata,Hyundai', 'Cars,Hyundai,Tata',\n",
       "       'tata,cars', 'Cars,Tata,Hyundai', 'cars,tata', 'Tata,Cars,tata',\n",
       "       'Cars,cars,Tata', 'TATA,Cars', 'Tata,Cars', 'Cars,Hyundai',\n",
       "       'TATA,cars', 'cars,Hyundai,Cars', 'hyundai,cars',\n",
       "       'cars,Hyundai,Cars,Tata', 'Tata,cars,Hyundai', 'TATA,CARS',\n",
       "       'cars,Hyundai,Tata', 'Tata,cars,Cars', 'cars,Tata,TATA,Hyundai',\n",
       "       'cars,TATA', 'Hyundai,cars,Tata,Cars', 'CARS,Hyundai,cars',\n",
       "       'tata,Tata,cars', 'Tata,Cars,tata,cars', 'Hyundai,Cars,cars',\n",
       "       'cars,hyundai,tata', 'Hyundai,cars,Tata', 'Hyundai,Tata,cars',\n",
       "       'Cars,cars,Tata,tata', 'Tata,Cars,cars', 'cars,Tata,tata',\n",
       "       'Hyundai,cars,hyundai', 'cars,hyundai', 'cars,Cars,Tata',\n",
       "       'cars,Hyundai,tata', 'cars,Tata,Hyundai,Cars', 'Cars,HYUNDAI',\n",
       "       'Tata,tata,cars', 'cars,Tata,Cars', 'cars,TATA,Tata',\n",
       "       'Tata,Cars,hyundai', 'Tata,cars,Hyundai,Cars', 'Hyundai,TATA,cars',\n",
       "       'Tata,cars,tata', 'TATA,cars,Tata', 'cars,tata,TATA',\n",
       "       'Tata,Hyundai,cars', 'cars,Hyundai,TATA', 'hyundai,cars,Hyundai',\n",
       "       'Tata,CARS,cars', 'TATA,tata,cars', 'Tata,cars,TATA', 'Cars,TATA',\n",
       "       'Tata,Cars,Hyundai,cars', 'cars,Hyundai,HYUNDAI,CARS',\n",
       "       'Cars,Tata,cars,Hyundai'], dtype=object)"
      ]
     },
     "execution_count": 1641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Keywords'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hyundai,cars', 'tata,cars', 'cars,tata', 'hyundai,cars,hyundai',\n",
       "       'cars,hyundai', 'hyundai,cars,cars', 'cars,tata,hyundai',\n",
       "       'cars,hyundai,tata', 'tata,cars,tata', 'cars,cars,tata',\n",
       "       'cars,hyundai,cars', 'cars,hyundai,cars,tata', 'tata,cars,hyundai',\n",
       "       'tata,cars,cars', 'cars,tata,tata,hyundai',\n",
       "       'hyundai,cars,tata,cars', 'tata,tata,cars', 'tata,cars,tata,cars',\n",
       "       'hyundai,cars,tata', 'hyundai,tata,cars', 'cars,cars,tata,tata',\n",
       "       'cars,tata,tata', 'cars,tata,hyundai,cars', 'cars,tata,cars',\n",
       "       'tata,cars,hyundai,cars', 'tata,hyundai,cars',\n",
       "       'cars,hyundai,hyundai,cars', 'cars,tata,cars,hyundai'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 1642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Keywords']=df['Keywords'].str.lower()\n",
    "df['Keywords'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1643,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce the data redundancy, I will set 3 categories tata, hyundai, tata_hyundai\n",
    "\n",
    "brand_keywords = {\n",
    "    'hyundai,cars':'hyundai', 'tata,cars':'tata', 'cars,tata':'tata', 'hyundai,cars,hyundai':'hyundai',\n",
    "       'cars,hyundai':'hyundai', 'hyundai,cars,cars':'hyundai', 'cars,tata,hyundai':'tata_hyundai',\n",
    "       'cars,hyundai,tata':'tata_hyundai', 'tata,cars,tata':'tata', 'cars,cars,tata':'tata',\n",
    "       'cars,hyundai,cars':'hyundai', 'cars,hyundai,cars,tata':'tata_hyundai', 'tata,cars,hyundai':'tata_hyundai',\n",
    "       'tata,cars,cars':'tata', 'cars,tata,tata,hyundai':'tata_hyundai',\n",
    "       'hyundai,cars,tata,cars':'tata_hyundai', 'tata,tata,cars':'tata', 'tata,cars,tata,cars':'tata',\n",
    "       'hyundai,cars,tata':'tata_hyundai', 'hyundai,tata,cars':'tata_hyundai', 'cars,cars,tata,tata':'tata',\n",
    "       'cars,tata,tata':'tata', 'cars,tata,hyundai,cars':'tata_hyundai', 'cars,tata,cars':'tata',\n",
    "       'tata,cars,hyundai,cars':'tata_hyundai', 'tata,hyundai,cars':'tata_hyundai',\n",
    "       'cars,hyundai,hyundai,cars':'hyundai', 'cars,tata,cars,hyundai':'tata_hyundai'\n",
    "}\n",
    "\n",
    "df['Keywords']=df['Keywords'].map(brand_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tata            984\n",
       "hyundai         477\n",
       "tata_hyundai     67\n",
       "Name: Keywords, dtype: int64"
      ]
     },
     "execution_count": 1644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Keywords'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                       0\n",
       "Headline                 818\n",
       "URL                        0\n",
       "Opening Text             818\n",
       "Hit Sentence               0\n",
       "Source                     0\n",
       "Influencer               463\n",
       "Language                   0\n",
       "Reach                      9\n",
       "Desktop Reach            818\n",
       "Mobile Reach             818\n",
       "Twitter Social Echo      818\n",
       "Facebook Social Echo     818\n",
       "Reddit Social Echo       818\n",
       "National Viewership        0\n",
       "Engagement              1176\n",
       "Sentiment                  0\n",
       "Keywords                   0\n",
       "State                    497\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1528\n",
      "Name: National Viewership, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['National Viewership'].value_counts())\n",
    "# Need to remove this variable \n",
    "df.drop(columns='National Viewership', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1647,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reach'].isna().sum() # It will make sense to remove these rows as there will not be much data loss\n",
    "\n",
    "df.dropna(subset='Reach', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will remove all the unnecessary columns with high null values\n",
    "df.drop(columns=['Desktop Reach', 'Mobile Reach', 'Twitter Social Echo', 'Reddit Social Echo', 'Facebook Social Echo', 'Opening Text', 'Headline'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date               0\n",
       "URL                0\n",
       "Hit Sentence       0\n",
       "Source             0\n",
       "Influencer       463\n",
       "Language           0\n",
       "Reach              0\n",
       "Engagement      1170\n",
       "Sentiment          0\n",
       "Keywords           0\n",
       "State            494\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@jayeshkhunt20      21\n",
      "@CSAlsundkar        20\n",
      "@MaheshKhokle       18\n",
      "@mr_singh1888       13\n",
      "Utkarsh Deshmukh    12\n",
      "                    ..\n",
      "@_autocrat           1\n",
      "@jayanth_s_iyer      1\n",
      "@thoughtsofSuraJ     1\n",
      "@mstock_in           1\n",
      "@poojashah0998       1\n",
      "Name: Influencer, Length: 761, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.Influencer.value_counts())\n",
    "# Looking at the value count of Influencer, we can remove this column as well.\n",
    "df.drop(columns='Influencer', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' State column is important because it can provide a lot of insights. \\nTherefore we can assign the null values with the value count ratio of the rest of the data\\nThe data has a total of 1519 observations and the null vales are 494, it is better to impute \\nthe missing values with the column names on the basis of value count ratios of remaining values'"
      ]
     },
     "execution_count": 1651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['State'].isna().sum()\n",
    "\n",
    "''' State column is important because it can provide a lot of insights. \n",
    "Therefore we can assign the null values with the value count ratio of the rest of the data\n",
    "The data has a total of 1519 observations and the null vales are 494, it is better to impute \n",
    "the missing values with the column names on the basis of value count ratios of remaining values'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the population ratios\n",
    "population_ratios = {\n",
    "    \"Tamil Nadu\": 0.3255,\n",
    "    \"Maharashtra\": 0.2693,\n",
    "    \"Delhi\": 0.1188,\n",
    "    \"Uttar Pradesh\": 0.1176,\n",
    "    \"Karnataka\": 0.0879,\n",
    "    \"Kerala\": 0.0532,\n",
    "    \"Gujarat\": 0.0433,\n",
    "    \"Telangana\": 0.0396,\n",
    "    \"Madhya Pradesh\": 0.0347,\n",
    "    \"Rajasthan\": 0.0272,\n",
    "    \"West Bengal\": 0.0260,\n",
    "    \"Haryana\": 0.0260,\n",
    "    \"Puducherry\": 0.0149,\n",
    "    \"Odisha\": 0.0136,\n",
    "    \"Andhra Pradesh\": 0.0124,\n",
    "    \"Chandigarh\": 0.0111,\n",
    "    \"Jharkhand\": 0.0099,\n",
    "    \"Punjab\": 0.0062,\n",
    "    \"Bihar\": 0.0062,\n",
    "    \"Jammu and Kashmir\": 0.0049,\n",
    "    \"Chhattisgarh\": 0.0049,\n",
    "    \"National\": 0.0037,\n",
    "    \"Assam\": 0.0025,\n",
    "    \"Uttarakhand\": 0.0025,\n",
    "    \"Arunachal Pradesh\": 0.0012,\n",
    "    \"Tripura\": 0.0012,\n",
    "    \"Goa\": 0.0012,\n",
    "    \"Mizoram\": 0.0012,\n",
    "    \"Himachal Pradesh\": 0.0012\n",
    "}\n",
    "\n",
    "# Function to impute null values in \"state\" column based on ratios\n",
    "def impute_state(row):\n",
    "    if pd.isna(row[\"State\"]):\n",
    "        # Generate a random value based on the population ratios\n",
    "        state = random.choices(list(population_ratios.keys()), list(population_ratios.values()))[0]\n",
    "        return state\n",
    "    else:\n",
    "        return row[\"State\"]\n",
    "\n",
    "# Apply the impute_state function to fill null values in the \"state\" column\n",
    "df[\"State\"] = df.apply(impute_state, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date               0\n",
       "URL                0\n",
       "Hit Sentence       0\n",
       "Source             0\n",
       "Language           0\n",
       "Reach              0\n",
       "Engagement      1170\n",
       "Sentiment          0\n",
       "Keywords           0\n",
       "State              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n",
    "\n",
    "# Considered & explored the option to impute all missing values through regression, \n",
    "# but it is not the best approach as most of the values in the  \n",
    "# engagement column are null (1170)\n",
    "\n",
    "# Also making assumptions and imputing the missing values with a specific number or ratio might not be the best practices,\n",
    "# so the best option is to remove the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1654,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Engagement', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempting Keyword Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1655,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Hit Scentence to lower case\n",
    "df['Hit Sentence']=df['Hit Sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the characters to remove: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Removing all the punctuations and special characters\n",
    "print('All the characters to remove:', string.punctuation)\n",
    "\n",
    "df['Hit Sentence']= df['Hit Sentence'].str.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1657,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the column\n",
    "df['Hit Sentence'] = df['Hit Sentence'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1658,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop words\n",
    "def remove_stop_words(text):\n",
    "    if isinstance(text, str):\n",
    "        words = nltk.word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        return text  # Return non-string data as is\n",
    "\n",
    "df['Hit Sentence'] = df['Hit Sentence'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral     846\n",
      "Positive    420\n",
      "Negative    253\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Frequency Count of each variable\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Sentiment: 0.10994075049374588\n",
      "Median Sentiment: 0.0\n",
      "Standard Deviation of Sentiment: 0.6566974779263589\n"
     ]
    }
   ],
   "source": [
    "sentiment_metric = {'Neutral':0,'Positive':1, 'Negative':-1}\n",
    "df['Sentiment Score'] = df['Sentiment'].map(sentiment_metric)\n",
    "\n",
    "# Sentiment Score analysis\n",
    "mean_sentiment = df['Sentiment Score'].mean()\n",
    "median_sentiment = df['Sentiment Score'].median()\n",
    "std_dev_sentiment = df['Sentiment Score'].std()\n",
    "\n",
    "print(\"Mean Sentiment:\", mean_sentiment)\n",
    "print(\"Median Sentiment:\", median_sentiment)\n",
    "print(\"Standard Deviation of Sentiment:\", std_dev_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Hit Sentence</th>\n",
       "      <th>Source</th>\n",
       "      <th>Language</th>\n",
       "      <th>Reach</th>\n",
       "      <th>State</th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>hyundai</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>tata</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>tata_hyundai</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>hyundai</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>tata</td>\n",
       "      <td>626</td>\n",
       "      <td>626</td>\n",
       "      <td>626</td>\n",
       "      <td>626</td>\n",
       "      <td>626</td>\n",
       "      <td>626</td>\n",
       "      <td>626</td>\n",
       "      <td>626</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>tata_hyundai</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Positive</td>\n",
       "      <td>hyundai</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Positive</td>\n",
       "      <td>tata</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Positive</td>\n",
       "      <td>tata_hyundai</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment      Keywords  Date  URL  Hit Sentence  Source  Language  Reach  \\\n",
       "0  Negative       hyundai   121  121           121     121       121    121   \n",
       "1  Negative          tata   127  127           127     127       127    127   \n",
       "2  Negative  tata_hyundai     5    5             5       5         5      5   \n",
       "3   Neutral       hyundai   190  190           190     190       190    190   \n",
       "4   Neutral          tata   626  626           626     626       626    626   \n",
       "5   Neutral  tata_hyundai    30   30            30      30        30     30   \n",
       "6  Positive       hyundai   162  162           162     162       162    162   \n",
       "7  Positive          tata   227  227           227     227       227    227   \n",
       "8  Positive  tata_hyundai    31   31            31      31        31     31   \n",
       "\n",
       "   State  Sentiment Score  parameters  \n",
       "0    121              121         121  \n",
       "1    127              127         127  \n",
       "2      5                5           5  \n",
       "3    190              190         190  \n",
       "4    626              626         626  \n",
       "5     30               30          30  \n",
       "6    162              162         162  \n",
       "7    227              227         227  \n",
       "8     31               31          31  "
      ]
     },
     "execution_count": 1743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Sentiment', 'Keywords'], as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
